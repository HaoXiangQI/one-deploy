- name: {{ api_name_id  }}  # unique api identifier, used for `watch` `delete` `update` etc.
  kind: RealtimeAPI
  predictor:
    type: python
    path: predictors/{{ predictor_file_py }}
    image: {{ image_for_inference  }}
    dependencies:
      pip: dependencies/{{ requirements_txt  }}
    server_side_batching:
      max_batch_size: 8
      batch_interval: 0.1s
    processes_per_replica: 4
    threads_per_process: 8
    config:
      use_cuda: {{ use_cuda  }}
      model_name: {{ model_name }}
      model_path: {{ model_path }}  # relative path in model-hub dir
    env:
      SERVICE_NAME: {{ api_name }}  # http api endpoint, different api_name_id share the same api_name
      SERVICE_NETWORK: "ai_service_proxy"  # default value ai_service_proxy
      SERVICE_TAGS: {{ api_name_tags }}
      SERVICE_LANG: {{ other_attribute }}
      SERVICE_CHECK_HTTP: "/"  # healthy check endpoint
      SERVICE_CHECK_INTERVAL: "15s"
      SERVICE_CHECK_INITIAL_STATUS: "critical"
      SERVICE_CHECK_DEREGISTER_AFTER: "3m"
  compute:
    gpu: {{ gpu_num }}  # mapping number of gpus into docker container (INT)
    local_gpus: {{ gpu_ids }} # host gpu ids to be mapping, split by comma (e.g. "1,3,4,7")
  networking:
    local_port: {{ random_port_num }} # host port number (INT)
    hostname: {{ api_name_id }}
